{"cells":[{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# important imports\n","# !pip install opencv-python\n","# !pip install patchify\n","# !pip install GPUtil\n","# !pip install torchsummary\n","# !pip install tqdm\n","# !pip install albumentations"]},{"cell_type":"code","execution_count":1,"metadata":{"_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","execution":{"iopub.execute_input":"2024-02-25T13:11:15.644821Z","iopub.status.busy":"2024-02-25T13:11:15.644419Z","iopub.status.idle":"2024-02-25T13:12:14.700287Z","shell.execute_reply":"2024-02-25T13:12:14.698956Z","shell.execute_reply.started":"2024-02-25T13:11:15.644787Z"},"trusted":true},"outputs":[{"name":"stderr","output_type":"stream","text":["/tmp/ipykernel_3871904/1322278246.py:15: DeprecationWarning: 'imghdr' is deprecated and slated for removal in Python 3.13\n","  import imghdr\n"]}],"source":["\n","from GPUtil import showUtilization as gpu_usage\n","\n","import numpy as np\n","import pandas as pd\n","import math\n","import matplotlib.pyplot as plt\n","# import seaborn as sns\n","import os\n","import cv2\n","import random\n","import glob\n","import PIL\n","from PIL import Image\n","from tqdm import tqdm\n","import imghdr\n","from patchify import patchify \n","\n","\n","import time\n","import torch\n","import torchvision\n","import torch.optim as optim\n","import albumentations as A\n","import torch.nn as nn\n","import torchvision.transforms.functional as TF\n","from torchvision import transforms\n","from torch.utils.data import Dataset, DataLoader\n","from torch.nn import ConvTranspose2d\n","from torch.nn import Conv2d\n","from torch.nn import MaxPool2d\n","from torch.nn import Module\n","from torch.nn import ModuleList\n","from torch.nn import ReLU\n","from torch.nn import Dropout\n","# from torchsummary import summary\n","from torchinfo import summary\n","\n","from torch.nn import BatchNorm2d \n","\n","from torchvision.transforms import CenterCrop\n","from torch.nn import functional as F\n","from torch.nn.functional import normalize"]},{"cell_type":"markdown","metadata":{},"source":["- source_tutorial: https://pyimagesearch.com/2021/11/08/u-net-training-image-segmentation-models-in-pytorch/\n","- unet: https://github.com/milesial/Pytorch-UNet\n","- Probablistic_unet: https://github.com/stefanknegt/Probabilistic-Unet-Pytorch/blob/master/probabilistic_unet.py\n","- utility script: https://github.com/CaptainDredge/Image-segmentation-utilities\n","- GPU_utility: https://github.com/anderskm/gputil\n","- torch_em: https://github.com/computational-cell-analytics/dl-for-micro/blob/main/2_cell_segmentation/torchem-train-cell-membrane-segmentation.ipynb\n","- torch_em: https://github.com/constantinpape/torch-em/blob/main/torch_em/model/unet.py\n","- digital_sreeni: https://github.com/bnsreenu/python_for_microscopists/blob/master/206_sem_segm_large_images_using_unet_with_custom_patch_inference.py\n","- data_Augmentatin: https://albumentations.ai/docs/getting_started/mask_augmentation/\n","- learning rate https://www.kaggle.com/code/isbhargav/guide-to-pytorch-learning-rate-scheduling\n","\n"]},{"cell_type":"markdown","metadata":{},"source":["### loss functionds\n","The validation loss function is just a metric and actually not needed for training. It's there because it make sense to compare the metrics which your network is actually optimzing on. So you can add any other loss function as metric during compilation and you'll see it during training.\n","- https://dev.to/_aadidev/3-common-loss-functions-for-image-segmentation-545o\n","- https://www.kaggle.com/code/yassinealouini/all-the-segmentation-metrics/notebook\n","- https://github.com/JunMa11/SegLoss \n","- https://pyimagesearch.com/2019/10/14/why-is-my-validation-loss-lower-than-my-training-loss/\n","- important for iunet loss and everytjing https://discuss.pytorch.org/t/unet-implementation/426/12"]},{"cell_type":"markdown","metadata":{},"source":["name coding: \n","bSz4_1CH_ip128_vls0.2_ADM_EnDc16-512_drp_ep10_aug4\n","\n","- bsz: batch size\n","- lR: learning rate 1e-num\n","- SDG: optimiser\n","- ADM: Adam optimiser\n","- clcLR: scheduler \n","- vls: validation split\n","- EnDc: encodert decoder channels\n","- pt: patience for earkly stopping\n","- i/p : input size \n","- ep: epochs\n","- wd: weight decay(l2 reg)\n","- drp: drop out added\n","- 1CH: input one channel instead of RGB\n","- aug: num of augmentation"]},{"cell_type":"markdown","metadata":{},"source":["## setting up cuda configiration for the deep learning network"]},{"cell_type":"code","execution_count":4,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["__Python VERSION: 3.11.2 (main, Mar 13 2023, 12:18:29) [GCC 12.2.0]\n","__pyTorch VERSION: 2.2.1+cu121\n","__CUDA VERSION\n","nvcc: NVIDIA (R) Cuda compiler driver\n","Copyright (c) 2005-2022 NVIDIA Corporation\n","Built on Wed_Sep_21_10:33:58_PDT_2022\n","Cuda compilation tools, release 11.8, V11.8.89\n","Build cuda_11.8.r11.8/compiler.31833905_0\n","__CUDNN VERSION: 8902\n","__Number CUDA Devices: 8\n","__Devices\n","index, name, driver_version, memory.total [MiB], memory.used [MiB], memory.free [MiB]\n","0, NVIDIA GeForce RTX 2080 Ti, 525.147.05, 11264 MiB, 8967 MiB, 2043 MiB\n","1, NVIDIA GeForce RTX 2080 Ti, 525.147.05, 11264 MiB, 1 MiB, 11010 MiB\n","2, NVIDIA GeForce RTX 2080 Ti, 525.147.05, 11264 MiB, 1 MiB, 11010 MiB\n","3, NVIDIA GeForce RTX 2080 Ti, 525.147.05, 11264 MiB, 1 MiB, 11010 MiB\n","4, NVIDIA GeForce RTX 2080 Ti, 525.147.05, 11264 MiB, 1 MiB, 11010 MiB\n","5, NVIDIA GeForce RTX 2080 Ti, 525.147.05, 11264 MiB, 1 MiB, 11010 MiB\n","6, NVIDIA GeForce RTX 2080 Ti, 525.147.05, 11264 MiB, 1 MiB, 11010 MiB\n","7, NVIDIA GeForce RTX 2080 Ti, 525.147.05, 11264 MiB, 1 MiB, 11010 MiB\n","Active CUDA Device: GPU 0\n","Available devices  8\n","Current cuda device  0\n","| ID | GPU | MEM |\n","------------------\n","|  0 |  0% | 80% |\n","|  1 |  0% |  0% |\n","|  2 |  0% |  0% |\n","|  3 |  0% |  0% |\n","|  4 |  0% |  0% |\n","|  5 |  0% |  0% |\n","|  6 |  0% |  0% |\n","|  7 |  0% |  0% |\n","None\n"]}],"source":["import torch\n","import sys\n","print('__Python VERSION:', sys.version)\n","print('__pyTorch VERSION:', torch.__version__)\n","print('__CUDA VERSION')\n","from subprocess import call\n","# call([\"nvcc\", \"--version\"]) does not work\n","! nvcc --version\n","print('__CUDNN VERSION:', torch.backends.cudnn.version())\n","print('__Number CUDA Devices:', torch.cuda.device_count())\n","print('__Devices')\n","call([\"nvidia-smi\", \"--format=csv\", \"--query-gpu=index,name,driver_version,memory.total,memory.used,memory.free\"])\n","print('Active CUDA Device: GPU', torch.cuda.current_device())\n","print ('Available devices ', torch.cuda.device_count())\n","print ('Current cuda device ', torch.cuda.current_device())\n","print(gpu_usage())"]},{"cell_type":"markdown","metadata":{},"source":["- https://stackoverflow.com/questions/54216920/how-to-use-multiple-gpus-in-pytorch\n","- https://stackoverflow.com/questions/59249563/runtimeerror-module-must-have-its-parameters-and-buffers-on-device-cuda1-devi"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":[]},{"cell_type":"code","execution_count":5,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["| ID | GPU | MEM |\n","------------------\n","|  0 |  0% | 80% |\n","|  1 |  0% |  0% |\n","|  2 |  0% |  0% |\n","|  3 |  0% |  0% |\n","|  4 |  0% |  0% |\n","|  5 |  0% |  0% |\n","|  6 |  0% |  0% |\n","|  7 |  0% |  0% |\n","None\n"]}],"source":["# torch.cuda.memory_reserved(1)\n","# useDevice = [1,2]\n","print(gpu_usage())\n","\n","      "]},{"cell_type":"markdown","metadata":{},"source":["## Setting up configuration for running in notebooks\n","\n","- The pin memory is set to True to the DataLoader which will automatically put the fetched data Tensors in pinned memory, enabling faster data transfer to CUDA-enabled GPU's. For every epoch the data is transferred from CPU to GPU, with augmentations done in the CPU, and trainings done in the GPU.\n","- Check the output directory for saving the images, model path, plot path and test path (BASE_OUTPUT, MODEL_PATH, PLOT_PATH, TEST_PATH)\n","- Note keep the bacthsize small (2,5) untill you start using pacthes of the image otherwise the GPU will run out of memory"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":[]},{"cell_type":"code","execution_count":6,"metadata":{},"outputs":[{"ename":"RuntimeError","evalue":"Invalid device string: 'cuda:0,1'","output_type":"error","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)","Cell \u001b[0;32mIn[6], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m devices \u001b[38;5;241m=\u001b[39m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdevice\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcuda:0,1\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcuda\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mis_available\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcpu\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28mprint\u001b[39m(devices)\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mcuda\u001b[38;5;241m.\u001b[39mdevice_count() \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m1\u001b[39m:\n","\u001b[0;31mRuntimeError\u001b[0m: Invalid device string: 'cuda:0,1'"]}],"source":["\n","devices = torch.device(\"cuda:0,1\" if torch.cuda.is_available() else \"cpu\")\n","print(devices)\n","if torch.cuda.device_count() > 1:\n","  print(\"Let's use\", torch.cuda.device_count(), \"GPUs!\")"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-02-25T13:14:44.803389Z","iopub.status.busy":"2024-02-25T13:14:44.802934Z","iopub.status.idle":"2024-02-25T13:14:44.816422Z","shell.execute_reply":"2024-02-25T13:14:44.81519Z","shell.execute_reply.started":"2024-02-25T13:14:44.803329Z"},"trusted":true},"outputs":[],"source":["# define the test split\n","# TEST_SPLIT = 0.3\n","VALIDATION_SPLIT = 0.4\n","\n","# determine the device to be used for training and evaluation\n","# DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n","# print('device', DEVICE)\n","# determine if we will be pinning memory during data loading\n","# PIN_MEMORY = True if DEVICE == \"cuda\" else False\n","\n","# define the number of inputchannels in the input, number of classes,\n","# and number of levels (start and end) in the U-Net model\n","NUM_CHANNELS = 1\n","NUM_CLASSES = 1\n","NUM_LEVELS = 3\n","START_CHANEL = 16\n","END_CHANEL = 256\n","NUM_WORKERS = 2 if torch.cuda.is_available() else 1 \n","\n","# initialize learning rate, number of epochs to train for, and the\n","# batch size ( in general small batch size has 256 samples, here due to 2 batch ss we have )\n","INIT_LR = 1e-3\n","NUM_EPOCHS = 10\n","BATCH_SIZE = 1\n","PATCH_SIZE = 256\n","WEIGHT_DECAY = 1e-6\n","\n","# define the input image dimensions\n","# INPUT_IMAGE_WIDTH = 2048\n","# INPUT_IMAGE_HEIGHT = 1536\n","\n","INPUT_IMAGE_WIDTH = PATCH_SIZE\n","INPUT_IMAGE_HEIGHT = PATCH_SIZE\n","NUM_AUGMENTATION = 3\n","TRANSFORMS = True\n","\n","#learning rate scheduler \n","Patience = 4\n","MAX_lr = 1e-1\n","BASE_lr = 1e-6\n","STEP_SIZE = 50\n","\n","# define thresholds for early stopping, for accuracy calculation and predcitions \n","EARLY_STOP_THRES = 3\n","best_accuracy = -1\n","best_epoch = -1\n","THRESHOLD = 0.5\n","\n","\n","plotName = f\"bSz{BATCH_SIZE}_ip{PATCH_SIZE}_EnDc{START_CHANEL}-{END_CHANEL}_ep{NUM_EPOCHS}_vls{VALIDATION_SPLIT}_aug{NUM_AUGMENTATION}\"\n","print(plotName)\n","\n","\n","BASE_OUTPUT = \"/user/s.chakrabarty/myoTwinWork/code/EmbryonicBodySegmentation\"\n","# define the path to the output serialized model, model training\n","# plot, and testing image paths\n","MODEL_PATH = os.path.join(BASE_OUTPUT, f\"{plotName}\")  # file_path = MODEL_PATH\n","# PLOT_PATH = os.path.sep.join([BASE_OUTPUT, \"plot.png\"])\n","# TEST_PATHS = os.path.sep.join([BASE_OUTPUT, \"test_paths.txt\"])\n","\n","# define the path to the base output directory\n","gpu_usage() "]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-02-25T10:44:18.258294Z","iopub.status.busy":"2024-02-25T10:44:18.256988Z","iopub.status.idle":"2024-02-25T10:44:18.268217Z","shell.execute_reply":"2024-02-25T10:44:18.26689Z","shell.execute_reply.started":"2024-02-25T10:44:18.258247Z"},"trusted":true},"outputs":[],"source":["import re\n","name = \"bSz2_ip128_EnDc16-256_ep10_vls0.3_aug3\"\n","# Define the regular expression pattern\n","pattern = r\"(ep\\d+)\"\n","# int(re.search(pattern, name).group(0))\n","\n","# Define the regular expression pattern to only capture the digits\n","pattern = r\"(ep\\d+)\"\n","\n","# Extract the number and convert it to an integer\n","lastEpochInfo = re.search(pattern, name)[0] # Extract the entire match\n","int(re.search(\"\\d+\", lastEpochInfo)[0]), lastEpochInfo\n","# print(\"Extracted number:\", number)"]},{"cell_type":"markdown","metadata":{},"source":["## CREATING THE CUSTOM DATASET"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-02-25T13:17:14.56475Z","iopub.status.busy":"2024-02-25T13:17:14.564336Z","iopub.status.idle":"2024-02-25T13:17:41.956829Z","shell.execute_reply":"2024-02-25T13:17:41.955643Z","shell.execute_reply.started":"2024-02-25T13:17:14.564721Z"},"trusted":true},"outputs":[],"source":["def readData(imgPath, labelPath, convertType):\n","    \"\"\"Reads and creates a list of target\"\"\" \n","    image = []\n","    label = []\n","    imageList = []\n","    labelList = []\n","  \n","    for i, image_name in enumerate(sorted(os.listdir(imgPath))):\n","        if ((('.').join(image_name.split('.')[-1:])== 'tif') or (('.').join(image_name.split('.')[-1:]) == 'tiff')):\n","            label_name = '.'.join(image_name.split('.')[:-1]) +  '_bn.tif'\n","            \n","            if label_name in list(os.listdir(labelPath)): \n","                # normalise by 255.0 -> convert to array -> append to list\n","                img_Path = os.path.join(imgPath, image_name)\n","                img = Image.open(img_Path).convert(convertType)\n","                img = np.array(img, dtype = np.float32)/255.0\n","                image.append(img)\n","                imageList.append((img_Path))\n","                \n","                label_Path = os.path.join(labelPath, label_name)\n","                img = Image.open(label_Path).convert(convertType)\n","                label.append(np.where(np.array(img) >= 1, 1.0, 0.0))\n","                labelList.append((label_Path))\n","            else:\n","                print('Images with no mask-->', image_name)\n","        else: print('Image with new extension', image_name)\n","    print(f'total images --> {len(imageList)}, total masks --> {len(labelList)}')       \n","    return image, label\n","            \n","input_folder = '/user/s.chakrabarty/myoTwinWork/data/embryonicImages/Images_input'\n","label_folder = '/user/s.chakrabarty/myoTwinWork/data/embryonicImages/Images_target'\n","image, label = readData(input_folder, label_folder, convertType = 'L')"]},{"cell_type":"markdown","metadata":{},"source":["/kaggle/input/myofarm/Images_target/20220714_JK_d1_diff_isRASb1_corr_13_5_uM_CHIR_220018_p17_6mio_dish_2_bn.tif\n","\n","'/kaggle/input/myofarm/Images_target/20230306_JK_230008_d12_isWT1Bld_2_261_bn.tif'\n","'/kaggle/input/myofarm/Images_target/20230217_JK_BRAF_mc_8_p23_d9_230007_bn.tif'"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-02-25T13:17:41.959068Z","iopub.status.busy":"2024-02-25T13:17:41.958733Z","iopub.status.idle":"2024-02-25T13:17:43.670424Z","shell.execute_reply":"2024-02-25T13:17:43.668915Z","shell.execute_reply.started":"2024-02-25T13:17:41.959039Z"},"trusted":true},"outputs":[],"source":["def plotSanityCheckImages(img1, img2):\n","    \n","    print(f'Images:{img1.shape} and mask:{img2.shape} shape\\n')\n","    \n","    figure, ax = plt.subplots(nrows=1, ncols=3, figsize=(10,10))\n","    ax[0].imshow(img1)\n","    ax[1].imshow(img2, cmap = 'gray')\n","    ax[2].imshow(img2 * img1, cmap = 'gray')\n","\n","    ax[0].set_title(\"Image with the original channel\")\n","    ax[1].set_title(\"Original Mask\")\n","    ax[2].set_title(\"combined and made gray\")\n","\n","    figure.tight_layout()\n","    figure.show()\n","\n","plotSanityCheckImages(image[0], label[0])"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-02-25T13:17:43.687485Z","iopub.status.busy":"2024-02-25T13:17:43.68691Z","iopub.status.idle":"2024-02-25T13:17:43.698742Z","shell.execute_reply":"2024-02-25T13:17:43.69742Z","shell.execute_reply.started":"2024-02-25T13:17:43.687432Z"},"trusted":true},"outputs":[],"source":["def dataTransform(image, mask):\n","    images_list, masks_list = [], []\n","    \n","    transform = A.Compose([ A.HorizontalFlip(p = 0.5),\n","                            A.VerticalFlip(p = 0.5),\n","                            A.RandomBrightnessContrast(p=0.5),\n","                            A.ElasticTransform(p=0.5),\n","                            A.GridDistortion(p = 0.5), ])\n","\n","    for i in range(NUM_AUGMENTATION):\n","        augmentations = transform(image = np.array(image), mask = np.array(mask))\n","        images_list.append(augmentations[\"image\"])\n","        masks_list.append(augmentations[\"mask\"])\n","        \n","    return images_list, masks_list\n","    \n","print('To check the augmentation patterns')\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-02-25T13:17:43.700766Z","iopub.status.busy":"2024-02-25T13:17:43.700332Z","iopub.status.idle":"2024-02-25T13:17:43.711568Z","shell.execute_reply":"2024-02-25T13:17:43.710375Z","shell.execute_reply.started":"2024-02-25T13:17:43.700724Z"},"trusted":true},"outputs":[],"source":["def createPatches(imgList, maskList, PATCH_SIZE):\n","    images = []\n","    masks = []\n","    for i, (image, mask) in enumerate(zip(imgList, maskList)):      \n","        patch_images = patchify(image, (PATCH_SIZE,PATCH_SIZE), step = PATCH_SIZE)\n","        patch_masks = patchify(mask, (PATCH_SIZE,PATCH_SIZE), step = PATCH_SIZE)\n","        \n","        for i in range(patch_images.shape[0]):\n","            for j in range(patch_images.shape[1]):\n","                single_patch_img = patch_images[i,j,:,:]\n","                images.append(single_patch_img)\n","                \n","                single_patch_img = patch_masks[i,j,:,:]\n","                masks.append(single_patch_img)\n","                \n","    images = torch.reshape(torch.tensor(np.array(images)), [-1,1,PATCH_SIZE,PATCH_SIZE])       \n","    masks = torch.reshape(torch.tensor(np.array(masks)), [-1,1,PATCH_SIZE,PATCH_SIZE])\n","        \n","    return images, masks    \n","    "]},{"cell_type":"markdown","metadata":{},"source":["Suppose NUM_AUGMENTATION is set to 15, and you have 10 original images in your dataset. So, the total number of items in the augmented dataset would be 10 * 15 = 150.\n","\n","When idx is 25:\n","original_idx = 25 // 15 = 1\n","augmentation_idx = 25 % 15 = 10\n","This means the current index corresponds to the 10th augmentation of the 2nd original image.\n","When idx is 43:\n","original_idx = 43 // 15 = 2\n","augmentation_idx = 43 % 15 = 13\n","This means the current index corresponds to the 13th augmentation of the 3rd original image.\n","\n","\n","he createPATCHES function likely processes multiple images and masks at once. By passing [augmented_img] and [augmented_mask] (which are lists), you're providing the function with the necessary format for handling multiple images and masks, even though in this case, you're passing only one of each."]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-02-25T13:17:43.713977Z","iopub.status.busy":"2024-02-25T13:17:43.713599Z","iopub.status.idle":"2024-02-25T13:17:43.730459Z","shell.execute_reply":"2024-02-25T13:17:43.729076Z","shell.execute_reply.started":"2024-02-25T13:17:43.713948Z"},"trusted":true},"outputs":[],"source":["class SpheroidDataset(Dataset):\n","\n","    def __init__(self, imagePaths, maskPaths, transforms = None, num_augmentations=NUM_AUGMENTATION):\n","        self.imagePaths = imagePaths\n","        self.maskPaths = maskPaths\n","        self.transforms = transforms\n","        self.num_augmentations = num_augmentations\n","\n","        # Read and store image and mask data efficiently\n","        self.images, self.masks = readData(self.imagePaths, self.maskPaths, \"L\")\n","\n","    def __len__(self):\n","        if self.transforms == True:\n","            length = len(self.images) * self.num_augmentations\n","        else:\n","            length = len(self.images)\n","        print(f'Images Augmentated-->{self.transforms}; so Dataset_length-->{length}')\n","        return length \n","    \n","\n","    def __getitem__(self, idx):\n","        if self.transforms == True:\n","            original_idx = idx // self.num_augmentations\n","            augmentation_idx = idx % self.num_augmentations\n","        else:\n","            original_idx = idx \n","\n","        # Retrieve image and mask from pre-loaded data\n","        img = self.images[original_idx]\n","        mask = self.masks[original_idx]\n","\n","        # Transform(y/n) -> patches \n","        if self.transforms == True:\n","            images_list, masks_list = dataTransform(img, mask)\n","            augmented_img = images_list[augmentation_idx]\n","            augmented_mask = masks_list[augmentation_idx]\n","            image_patch, mask_patch = createPatches([augmented_img], [augmented_mask], PATCH_SIZE)\n","        else:\n","            image_patch, mask_patch = createPatches([img], [mask], PATCH_SIZE)\n","\n","        return image_patch, mask_patch \n","\n","\n","# input_folder = '/kaggle/input/myofarm/Images_input'\n","# mask_folder = '/kaggle/input/myofarm/Images_target'\n","\n","# dataset = SpheroidDataset(input_folder, mask_folder, transforms = True)\n","\n","# for images, masks in dataset:    \n","#     print(images.shape) #print path and others also for checking\n","#     print(torch.min(images), torch.mean(images), torch.max(images))\n","#     print(masks.shape)\n","#     print('\\n')\n","#     break\n","    \n","# print(\"Total images: \", len(dataset))"]},{"cell_type":"markdown","metadata":{},"source":["Images with no mask--> 20230217_JK_BRAF_mc_8_p23_d9_230007.tif\n","Images with no mask--> 20230306_JK_230008_d12_isWT1Bld_2_261.tif\n","total images --> 122\n","total masks --> 122\n","transforming\n","torch.Size([192, 1, 128, 128])\n","tensor(0.2118) tensor(0.5156) tensor(0.9176)\n","torch.Size([192, 1, 128, 128])\n","\n","\n","original_length-->122\n","augmented_length-->488\n","Total images:  488"]},{"cell_type":"markdown","metadata":{},"source":["## LOSS Functions and Accuracy metric\n"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-02-25T13:17:43.732576Z","iopub.status.busy":"2024-02-25T13:17:43.732149Z","iopub.status.idle":"2024-02-25T13:17:43.745223Z","shell.execute_reply":"2024-02-25T13:17:43.744311Z","shell.execute_reply.started":"2024-02-25T13:17:43.73254Z"},"trusted":true},"outputs":[],"source":["def modelAccuracy(inputs, targets):\n","    # Calculate pixel-wise accuracy\n","    # assuming input and targets are falttened\n","    correct = (inputs == targets).float()\n","    accuracy = correct.sum() / targets.numel()\n","    \n","    return accuracy"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-02-25T13:17:43.747431Z","iopub.status.busy":"2024-02-25T13:17:43.746493Z","iopub.status.idle":"2024-02-25T13:17:43.756793Z","shell.execute_reply":"2024-02-25T13:17:43.75547Z","shell.execute_reply.started":"2024-02-25T13:17:43.747388Z"},"trusted":true},"outputs":[],"source":["class DiceLoss(nn.Module):\n","    def __init__(self, weight=None, size_average=True):\n","        super(DiceLoss, self).__init__()\n","\n","    def forward(self, inputs, targets, smooth=1):\n","        \n","        #comment out if your model contains a sigmoid or equivalent activation layer\n","        # inputs = F.sigmoid(inputs)       \n","        \n","        #flatten label and prediction tensors\n","        inputs = inputs.view(-1)\n","        targets = targets.view(-1)\n","        \n","        intersection = (inputs * targets).sum()                            \n","        dice = (2.*intersection + smooth)/(inputs.sum() + targets.sum() + smooth) \n","        accuracy = modelAccuracy(inputs, targets)\n","        \n","        return 1 - dice, accuracy"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-02-25T13:17:43.759246Z","iopub.status.busy":"2024-02-25T13:17:43.758793Z","iopub.status.idle":"2024-02-25T13:17:43.771108Z","shell.execute_reply":"2024-02-25T13:17:43.76973Z","shell.execute_reply.started":"2024-02-25T13:17:43.759211Z"},"trusted":true},"outputs":[],"source":["#PyTorch\n","class DiceBCELoss(nn.Module):\n","    def __init__(self, weight=None, size_average=True):\n","        super(DiceBCELoss, self).__init__()\n","\n","    def forward(self, inputs, targets, smooth=1):\n","        \n","        #comment out if your model contains a sigmoid or equivalent activation layer\n","        # inputs = F.sigmoid(inputs)       \n","        \n","        #flatten label and prediction tensors\n","        inputs = inputs.view(-1)\n","        targets = targets.view(-1)\n","        \n","        intersection = (inputs * targets).sum()                            \n","        dice_loss = 1 - (2.*intersection + smooth)/(inputs.sum() + targets.sum() + smooth)  \n","        BCE = F.binary_cross_entropy(inputs, targets, reduction='mean')\n","        Dice_BCE = BCE + dice_loss\n","        \n","        accuracy = modelAccuracy(inputs, targets)\n","        \n","        return Dice_BCE, accuracy"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-02-25T13:17:43.775315Z","iopub.status.busy":"2024-02-25T13:17:43.774651Z","iopub.status.idle":"2024-02-25T13:17:43.784585Z","shell.execute_reply":"2024-02-25T13:17:43.783322Z","shell.execute_reply.started":"2024-02-25T13:17:43.775269Z"},"trusted":true},"outputs":[],"source":["#PyTorch\n","ALPHA = 0.8\n","GAMMA = 2\n","\n","class FocalLoss(nn.Module):\n","    # loss = model_lossFunc(pred.to(torch.float32), y.to(torch.float32))\n","    def __init__(self, weight=None, size_average=True):\n","        super(FocalLoss, self).__init__()\n","\n","    def forward(self, inputs, targets, alpha=ALPHA, gamma=GAMMA, smooth=1):\n","        #comment out if your model contains a sigmoid or equivalent activation layer\n","        # inputs = F.sigmoid(inputs)       \n","        \n","        #flatten label and prediction tensors\n","        inputs = inputs.view(-1) #model_predictions\n","        targets = targets.view(-1) #originalMasks\n","        \n","        #first compute binary cross-entropy \n","        BCE = F.binary_cross_entropy(inputs, targets, reduction='mean')\n","        BCE_EXP = torch.exp(-BCE)\n","        focal_loss = alpha * (1-BCE_EXP)**gamma * BCE\n","        \n","        accuracy = modelAccuracy(inputs, targets)\n","                       \n","        return focal_loss, accuracy"]},{"cell_type":"markdown","metadata":{},"source":["## Building UNET\n","\n","- Overall, our U-Net model will consist of an Encoder class and a Decoder class. The encoder will gradually reduce the spatial dimension to compress information. Furthermore, it will increase the number of channels, that is, the number of feature maps at each stage, enabling our model to capture different details or features in our image. On the other hand, the decoder will take the final encoder representation and gradually increase the spatial dimension and reduce the number of channels to finally output a segmentation mask of the same spatial dimension as the input image.\n","\n","- Next, we define a Block module as the building unit of our encoder and decoder architecture. It is worth noting that all models or model sub-parts that we define are required to inherit from the PyTorch Module class, which is the parent class in PyTorch for all neural network modules."]},{"cell_type":"markdown","metadata":{},"source":["## trial model with channel info outside "]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# device = 'cuda:3'\n","with torch.no_grad():\n","    torch.cuda.empty_cache()\n","print(gpu_usage())\n","DEVICE = torch.device(\"cuda:1\" if torch.cuda.is_available() else \"cpu\")\n","print(DEVICE)\n","# summary(UNet().to(device), input_size=(1,PATCH_SIZE,PATCH_SIZE))\n","# model = UNet()\n","# model = nn.DataParallel(model, device_ids=[3, 2, 1]).to(device)\n","print(gpu_usage())\n"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-02-25T13:17:43.786423Z","iopub.status.busy":"2024-02-25T13:17:43.786052Z","iopub.status.idle":"2024-02-25T13:17:43.799319Z","shell.execute_reply":"2024-02-25T13:17:43.798081Z","shell.execute_reply.started":"2024-02-25T13:17:43.786392Z"},"trusted":true},"outputs":[],"source":["def UNETchannels(startChanel, endChanel, num_chanel = NUM_CHANNELS):\n","    startPower = int(math.log2(startChanel))\n","    endPower = int(math.log2(endChanel))\n","    enCh = [num_chanel]\n","    deCh = []\n","    for num in range(startPower, endPower+1):\n","        enCh.append(2**num)\n","    deCh = enCh[::-1]\n","    deCh.pop()\n","    return tuple(enCh), tuple(deCh)\n","\n","# START_CHANEL = 16\n","# END_CHANEL = 1024\n","enCh, deCh = UNETchannels(START_CHANEL, END_CHANEL)"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-02-25T13:17:43.801876Z","iopub.status.busy":"2024-02-25T13:17:43.801035Z","iopub.status.idle":"2024-02-25T13:17:43.812163Z","shell.execute_reply":"2024-02-25T13:17:43.811171Z","shell.execute_reply.started":"2024-02-25T13:17:43.801831Z"},"trusted":true},"outputs":[],"source":["class Block(Module):\n","    def __init__(self, inChannels, outChannels):\n","        super().__init__()\n","        self.conv1 = nn.Conv2d(inChannels, outChannels,\n","                            kernel_size=3, stride=1, padding=1, bias=True)\n","        self.BN1 = nn.BatchNorm2d(outChannels)\n","        self.relu1 = nn.ReLU(inplace = True)\n","        self.dropout = nn.Dropout(0.25)\n","            \n","        self.conv2 = Conv2d(outChannels, outChannels, \n","                            kernel_size=3, stride=1, padding=1, bias=True)\n","        self.BN2 = BatchNorm2d(outChannels)\n","        self.relu2 = ReLU(inplace = True)\n","        \n","    def forward(self, x):\n","        # apply CONV => [BN] => RELU => CONV block to the inputs and return it\n","        outputConv1 = self.dropout(self.relu1(self.BN1(self.conv1(x))))\n","        outputConv2 = self.relu2(self.BN2(self.conv2(outputConv1))) \n","        return outputConv2"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-02-25T13:17:43.814634Z","iopub.status.busy":"2024-02-25T13:17:43.814149Z","iopub.status.idle":"2024-02-25T13:17:44.233301Z","shell.execute_reply":"2024-02-25T13:17:44.232187Z","shell.execute_reply.started":"2024-02-25T13:17:43.814593Z"},"trusted":true},"outputs":[],"source":["class Encoder(Module):    \n","    def __init__(self, channels = enCh): \n","        super().__init__()\n","        self.encBlocks = ModuleList([Block(channels[i], channels[i + 1]) \n","                                      for i in range(len(channels) - 1)])\n","        self.pool = MaxPool2d(kernel_size=2, stride=2)\n","\n","    # this takes an input image x\n","    # initialize an empty list (blockOutputs) to store the intermediate outputs passed\n","    # to the decoder where they can be processed with the decoder feature maps.   \n","    def forward(self, x): \n","        blockOutputs = []\n","        for block in self.encBlocks:\n","            x = block(x)\n","            blockOutputs.append(x)\n","            x = self.pool(x)\n","\n","        return blockOutputs\n","    \n","    \n","class Decoder(Module):\n","    def __init__(self, channels = deCh):\n","        super().__init__()\n","\n","        self.channels = channels\n","        self.upconvs = ModuleList([ConvTranspose2d(channels[i], channels[i + 1], kernel_size=2, stride=2)\n","                                   for i in range(len(channels) - 1)])\n","        self.dec_blocks = ModuleList([Block(channels[i], channels[i + 1])\n","                                       for i in range(len(channels) - 1)])\n","\n","    # Input is the feature map x and \n","    # the list of intermediate outputs from the encoder \n","    def forward(self, x, encFeatures):\n","\n","        for i in range(len(self.channels) - 1):\n","            x = self.upconvs[i](x)\n","            # crop the current features from the encoder blocks,\n","            # concatenate them with the current upsampled features,\n","            # and pass the concatenated output through the current decoder block\n","            # encFeat = self.crop(encFeatures[i], x)\n","            x = torch.cat([x, encFeatures[i]], dim=1)\n","            x = self.dec_blocks[i](x)\n","\n","        return x  \n","\n","class UNet(Module):\n","        \n","    def __init__(self, encChannels = enCh,\n","                 decChannels = deCh, \n","                 nbClasses=1, retainDim=True,\n","                 outSize=(INPUT_IMAGE_HEIGHT,  INPUT_IMAGE_WIDTH)):\n","        super().__init__()\n","        self.encoder = Encoder(encChannels)\n","        self.decoder = Decoder(decChannels)\n","        self.head = nn.Sequential(nn.Conv2d(decChannels[-1], nbClasses, kernel_size=1),\n","                                  nn.Sigmoid() )\n","        self.retainDim = retainDim\n","        self.outSize = outSize\n","        \n","        \n","    def forward(self, x):\n","        # grab the features from the encoder\n","        # Note that the encFeatures list contains \n","        # all the feature maps starting from the first encoder block output to the last\n","        encFeatures = self.encoder(x)\n","        # pass the encoder features through decoder making sure that\n","        # their dimensions are suited for concatenation\n","        # since the encoder feature maps starting from the last encoder block output to the first of the decoder\n","       \n","        # output of the final encoder block \n","        # (i.e., encFeatures[::-1][0]) and the feature map outputs of all intermediate encoder blocks \n","        # (i.e., encFeatures[::-1][1:]) to the decoder\n","        decFeatures = self.decoder(encFeatures[::-1][0], encFeatures[::-1][1:])\n","        map = self.head(decFeatures)\n","        \n","        if self.retainDim:\n","            map = F.interpolate(map, self.outSize)\n","\n","        return map   \n","# summary(UNet().to(DEVICE), input_size=(1,PATCH_SIZE,PATCH_SIZE))"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# # device = 'cuda:3'\n","# with torch.no_grad():\n","#     torch.cuda.empty_cache()\n","# print(gpu_usage())\n","# device = torch.device(\"cuda:1\" if torch.cuda.is_available() else \"cpu\")\n","# print(device)\n","# # summary(UNet().to(device), input_size=(1,PATCH_SIZE,PATCH_SIZE), device = \"cuda\")\n","# # model = UNet()\n","# # model = nn.DataParallel(model, device_ids=[3, 2, 1]).to(device)\n","# print(gpu_usage())\n"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":[]},{"cell_type":"markdown","metadata":{},"source":["- nbClasses: This defines the number of segmentation classes where we have to classify each pixel. This usually corresponds to the number of channels in our output segmentation map, where we have one channel for each class. Since we are working with two classes (i.e., binary classification), we keep a single channel and use thresholding for classification, as we will discuss later.\n","\n","- Now the encFeatures[::-1] list contains the feature map outputs in reverse order (i.e., from the last to the first encoder block). Note that this is important since, on the decoder side, we will be utilizing the encoder feature maps starting from the last encoder block output to the first."]},{"cell_type":"markdown","metadata":{},"source":["## load previously trained model\n"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# def loadOldModel(path, newModel = None):\n","#     if newModel == True:\n","        "]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-02-25T14:03:46.488371Z","iopub.status.busy":"2024-02-25T14:03:46.487938Z","iopub.status.idle":"2024-02-25T14:03:46.504712Z","shell.execute_reply":"2024-02-25T14:03:46.503748Z","shell.execute_reply.started":"2024-02-25T14:03:46.48834Z"},"trusted":true},"outputs":[],"source":["# import os #/kaggle/input/test_myotwin/pytorch/unet-bsz2_ip128_endc16-256_ep10/1/unet_spheroid-3.pth\n","# os.listdir(\"/kaggle/input/test_myotwin/pytorch/v2fromver56/2\") #/kaggle/input/test_myotwin/pytorch/v2fromver56/2\n","# os.list(\"/kaggle/input/test_myotwin/pytorch/v2fromver56/2/bSz2_ip128_EnDc16-256_ep10_vls0.3_aug3_FullModel.pth\")"]},{"cell_type":"markdown","metadata":{},"source":["## checkpoints and saving of model "]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-02-25T13:17:44.236302Z","iopub.status.busy":"2024-02-25T13:17:44.235151Z","iopub.status.idle":"2024-02-25T13:17:44.245007Z","shell.execute_reply":"2024-02-25T13:17:44.243797Z","shell.execute_reply.started":"2024-02-25T13:17:44.236258Z"},"trusted":true},"outputs":[],"source":["# this is just to save the model parameters not very comprehensive\n","def checkpoint_inference(model, epoch, optimizer, accLoss_info, file_path = None):\n","    # file_path = MODEL_PATH\n","    if file_path == None:\n","        file_path = pwd() + '_ModelInference.pth'\n","    else: file_path = file_path + '_ModelInference.pth'\n","    print(\" =====> Saving all model parameters for inference since there is improvement in model accuracy\")\n","    torch.save({\n","            'model': model,\n","            'model_state_dict': model.state_dict(),\n","            'epoch': epoch,\n","            'optimizer_state_dict': optimizer.state_dict(),\n","            'acc_loss_info': accLoss_info}, file_path)\n","    \n"," # this is for the full model saving \n","def save_checkpoint(model, file_path = None):\n","    print(\" =====> Saving the complete model\")\n","    if file_path == None: \n","        file_path = pwd() + '_FullModel.pth'\n","    else: \n","        file_path = file_path + '_FullModel.pth'\n","    torch.save(model, file_path)\n","    return file_path"]},{"cell_type":"markdown","metadata":{},"source":["## Train the UNET model"]},{"cell_type":"markdown","metadata":{},"source":["- ToPILImage(): it enables us to convert our input images to PIL image format. Note that this is necessary since we used OpenCV to load images in our custom dataset, but PyTorch expects the input image samples to be in PIL format.\n","- Resize(): allows us to resize our images to a particular input dimension (i.e., config.INPUT_IMAGE_HEIGHT, config.INPUT_IMAGE_WIDTH) that our model can accept\n","- ToTensor(): enables us to convert input images to PyTorch tensors and convert the input PIL Image, which is originally in the range from [0, 255], to [0, 1].\n"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-02-25T13:17:44.247041Z","iopub.status.busy":"2024-02-25T13:17:44.246543Z","iopub.status.idle":"2024-02-25T13:17:53.665022Z","shell.execute_reply":"2024-02-25T13:17:53.664009Z","shell.execute_reply.started":"2024-02-25T13:17:44.247006Z"},"trusted":true},"outputs":[],"source":["# BATCH_SIZE = 1\n","# # VALIDATION_SPLIT = 0.2\n","# NUM_WORKERS = 16\n","train_dataset = SpheroidDataset(input_folder, label_folder, transforms = False)\n","train_set, validation_set = torch.utils.data.random_split(train_dataset, \n","                                                          [round(len(train_dataset)*(1-VALIDATION_SPLIT)), \n","                                                           round(len(train_dataset)*VALIDATION_SPLIT)])\n","\n","trainLoader = DataLoader(dataset = train_set, batch_size = BATCH_SIZE, num_workers = NUM_WORKERS, shuffle =True)\n","validationLoader = DataLoader(dataset = validation_set, batch_size = BATCH_SIZE, num_workers = NUM_WORKERS, shuffle =True) \n","\n","# calculate steps per epoch for training and test set\n","trainSteps = len(train_set) // BATCH_SIZE\n","testSteps = len(validation_set) // BATCH_SIZE\n","validationSteps = len(validation_set) // BATCH_SIZE\n","\n","print(f\"Total {len(train_set)} instances in the training and {trainSteps} per epoch\")\n","print(f\"Total {len(validation_set)} instances in the validation set and {validationSteps} per epoch\")\n","print(f\"Each Epoch runs: {len(trainLoader)} times, where each loader has {BATCH_SIZE} instances for trainig\")\n","print(f\"ModelName-{plotName}\")\n","\n","gpu_usage() "]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-02-25T13:17:53.666908Z","iopub.status.busy":"2024-02-25T13:17:53.666367Z","iopub.status.idle":"2024-02-25T13:17:53.675981Z","shell.execute_reply":"2024-02-25T13:17:53.674215Z","shell.execute_reply.started":"2024-02-25T13:17:53.666878Z"},"trusted":true},"outputs":[],"source":["# import re\n","# name = \"bSz2_ip128_EnDc16-256_ep10_vls0.3_aug3\"\n","# # Define the regular expression pattern\n","# pattern = r\"(ep\\d+)\"\n","# # int(re.search(pattern, name).group(0))\n","\n","# # Define the regular expression pattern to only capture the digits\n","# pattern = r\"(ep\\d+)\"\n","\n","# # Extract the number and convert it to an integer\n","# lastEpochInfo = re.search(pattern, name)[0] # Extract the entire match\n","# int(re.search(\"\\d+\", lastEpochInfo)[0]), lastEpochInfo\n","# # print(\"Extracted number:\", number)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":[]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-02-25T10:56:23.765488Z","iopub.status.busy":"2024-02-25T10:56:23.765053Z","iopub.status.idle":"2024-02-25T10:56:23.773083Z","shell.execute_reply":"2024-02-25T10:56:23.771926Z","shell.execute_reply.started":"2024-02-25T10:56:23.765455Z"},"trusted":true},"outputs":[],"source":["# oldModel = str(os.listdir('/kaggle/input/test_myotwin/pytorch'))\n","# pattern = r\"(ep\\d+)\"\n","# trainedEpoch = re.search(pattern, oldModel)[0]\n","# trainedEpoch = re.search(\"\\d+\", lastEpochInfo)[0]\n","# print(trainedEpoch)"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-02-25T10:45:52.812874Z","iopub.status.busy":"2024-02-25T10:45:52.812474Z","iopub.status.idle":"2024-02-25T10:45:52.818782Z","shell.execute_reply":"2024-02-25T10:45:52.817608Z","shell.execute_reply.started":"2024-02-25T10:45:52.812828Z"},"trusted":true},"outputs":[],"source":["# newModel = True\n","# path = '/kaggle/input/test_myotwin/pytorch/unet-bsz2_ip128_endc16-256_ep10/1/unet_spheroid-3.pth'\n","# if newModel == True:\n","# #     unet = torch.load(path).to(DEVICE)\n","#     print('Model Loaded')"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["import torch\n","import sys\n","print('__Python VERSION:', sys.version)\n","print('__pyTorch VERSION:', torch.__version__)\n","print('__CUDA VERSION')\n","from subprocess import call\n","# call([\"nvcc\", \"--version\"]) does not work\n","! nvcc --version\n","print('__CUDNN VERSION:', torch.backends.cudnn.version())\n","print('__Number CUDA Devices:', torch.cuda.device_count())\n","print('__Devices')\n","call([\"nvidia-smi\", \"--format=csv\", \"--query-gpu=index,name,driver_version,memory.total,memory.used,memory.free\"])\n","print('Active CUDA Device: GPU', torch.cuda.current_device())\n","print ('Available devices ', torch.cuda.device_count())\n","print ('Current cuda device ', torch.cuda.current_device())"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# if torch.cuda.device_count() > 1:\n","#   print(\"Let's use\", torch.cuda.device_count(), \"GPUs!\")"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":[]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-02-20T14:40:17.644994Z","iopub.status.busy":"2024-02-20T14:40:17.64381Z","iopub.status.idle":"2024-02-20T14:40:17.87808Z","shell.execute_reply":"2024-02-20T14:40:17.877161Z","shell.execute_reply.started":"2024-02-20T14:40:17.644962Z"},"trusted":true},"outputs":[],"source":["# OLD ONE\n","# initialize loss function and optimizerimizer\n","# https://discuss.pytorch.org/t/how-to-specify-a-gpu-as-the-main-gpu-in-dataparallel/154375/3\n","# unet = UNet()\n","# unet = nn.DataParallel(unet, device_ids=[2,3])\n","# # unet = UNet().to(DEVICE)\n","# unet.to(DEVICE)\n","device = torch.device(\"cuda:1\" if torch.cuda.is_available() else \"cpu\")\n","unet = UNet()\n","unet = nn.DataParallel(unet, device_ids=[1, 2, 3]).to(device)\n","print(gpu_usage())\n","\n","metric_lossFunc = DiceBCELoss() #nn.BCELoss() #DiceLoss()\n","optimizer = optim.Adam(unet.parameters(), lr= INIT_LR, weight_decay = WEIGHT_DECAY)\n","\n","\n","acc_loss_info = {\"train_loss\": [], \"validation_loss\": [], \"model_accuracy\":[]}\n","gpu_usage() \n"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-02-25T09:36:11.013252Z","iopub.status.busy":"2024-02-25T09:36:11.012781Z","iopub.status.idle":"2024-02-25T09:36:11.019317Z","shell.execute_reply":"2024-02-25T09:36:11.018001Z","shell.execute_reply.started":"2024-02-25T09:36:11.01322Z"},"trusted":true},"outputs":[],"source":["# OLD ONE\n","import os\n","print(f\"ModelName:-{plotName}\")\n","os.environ[\"NCCL_DEBUG\"] = \"INFO\"\n","print(\"[INFO] training the network...\\nwith the gou status\", gpu_usage())\n","startTime = time.time()\n","_i = 0 # to check if the early stop works or not \n","\n","for epoch in tqdm(range(NUM_EPOCHS)):\n","    unet.train()\n","    totalTrainLoss = 0\n","    totalValidationLoss_model = 0\n","    totalValidationLoss_metric = 0\n","    totalAccuracy = 0\n","    for (i, (x, y)) in enumerate(trainLoader):\n","        x = torch.reshape(x,[-1,1,PATCH_SIZE,PATCH_SIZE]).float()\n","        y = torch.reshape(y,[-1,1,PATCH_SIZE,PATCH_SIZE]).float()\n","        (x, y) = (x.to(DEVICE), y.to(DEVICE))\n","        #print(x.shape)\n","\n","        pred = unet(x) \n","        loss, accuracy = metric_lossFunc(pred.to(torch.float32), y.to(torch.float32))\n","        optimizer.zero_grad()\n","        loss.backward()\n","        optimizer.step()\n","        totalTrainLoss += loss.item()\n","        \n","    with torch.no_grad():\n","        unet.eval()\n","        for (x, y) in validationLoader:\n","            \"\"\" Note make sure to change the i/p channel to later on ([-1, 1, patch_size, patch_size])\"\"\"\n","            x = torch.reshape(x,[-1,1,PATCH_SIZE,PATCH_SIZE]).float()\n","            y = torch.reshape(y,[-1,1,PATCH_SIZE,PATCH_SIZE]).float()\n","            (x, y) = (x.to(DEVICE), y.to(DEVICE))\n","            pred = unet(x)\n","            loss, acc = metric_lossFunc(pred.to(torch.float32), y.to(torch.float32))\n","            totalValidationLoss_metric += loss.item()\n","            totalAccuracy += acc.item()\n","            \n","    # calculate the average training and validation loss and binary accuracy\n","    avgTrainLoss = totalTrainLoss / trainSteps\n","    avgValidationLoss = totalValidationLoss_metric / testSteps\n","    avgAcc = totalAccuracy / testSteps\n","    \n","    acc_loss_info[\"train_loss\"].append(avgTrainLoss)\n","    acc_loss_info[\"validation_loss\"].append(avgValidationLoss) \n","    acc_loss_info[\"model_accuracy\"].append(avgAcc) \n","    \n","    # print the model training and validation information\n","    print(\"[INFO] EPOCH: {}/{}\".format(epoch + 1, NUM_EPOCHS))\n","    print(f\"Train loss: {avgTrainLoss:.4f} \\nValidation loss:{avgValidationLoss:.4f} \\nVal accuracy: {avgAcc:.4f}%\")\n","\n","\n","    # best_accuracy = -1 defined above \n","    # terminate the training loop\n","    if avgAcc - _i > best_accuracy:\n","        best_accuracy = avgAcc\n","        best_epoch = epoch\n","        checkpoint_inference(unet, epoch, optimizer, acc_loss_info, MODEL_PATH)\n","        save_checkpoint(unet, MODEL_PATH)\n","        _i = -1\n","\n","    elif epoch - best_epoch > EARLY_STOP_THRES:\n","        print(\"Early stopped training at epoch %d\" % epoch)\n","        break \n","    \n","endTime = time.time()\n","print(\"[INFO] total time taken to train the model: {:.2f}s\".format(endTime - startTime))"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["pwd()"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["gpu_usage() "]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-02-25T13:19:37.919937Z","iopub.status.busy":"2024-02-25T13:19:37.919501Z","iopub.status.idle":"2024-02-25T13:19:37.939956Z","shell.execute_reply":"2024-02-25T13:19:37.93886Z","shell.execute_reply.started":"2024-02-25T13:19:37.919908Z"},"trusted":true},"outputs":[],"source":["# loop over epochs\n","print(\"[INFO] training the network...\")\n","startTime = time.time()\n","for epoch in tqdm(range(NUM_EPOCHS)):\n","    # set the model in training mode - initialise train and val loss - loop on train set\n","    unet.train()\n","    totalTrainLoss = 0\n","    totalValidationLoss_model = 0\n","    totalValidationLoss_metric = 0\n","    totalAccuracy = 0\n","    print(f\"the model runs -> {len(trainLoader)} per epoch {epoch}\")\n","    \n","    for (i, (x, y)) in enumerate(trainLoader):\n","        # print(f\"\\ntrainLoader enumerated #_{i}_section\\n\")\n","        # send the input to the device - forward pass - train loss - zero out prev gradients \n","        # - back propagation - update model params - add train loss (.item makes it float)\n","        #\"\"\" Note make sure to change the i/p channel to later on ([-1, 1, patch_size, patch_size])\"\"\"\n","        x = torch.reshape(x,[-1,1,PATCH_SIZE,PATCH_SIZE]).float()\n","        y = torch.reshape(y,[-1,1,PATCH_SIZE,PATCH_SIZE]).float()\n","        (x, y) = (x.to(DEVICE), y.to(DEVICE))\n","        #print(x.shape)\n","        pred = unet(x) \n","        loss, accuracy = metric_lossFunc(pred.to(torch.float32), y.to(torch.float32))\n","        optimizer.zero_grad()\n","        loss.backward()\n","        optimizer.step()\n","        totalTrainLoss += loss.item()\n","        \n","    # switch off autograd for validation set\n","    with torch.no_grad():\n","        unet.eval()\n","        for (x, y) in validationLoader:\n","            \"\"\" Note make sure to change the i/p channel to later on ([-1, 1, patch_size, patch_size])\"\"\"\n","            x = torch.reshape(x,[-1,1,PATCH_SIZE,PATCH_SIZE]).float()\n","            y = torch.reshape(y,[-1,1,PATCH_SIZE,PATCH_SIZE]).float()\n","            (x, y) = (x.to(DEVICE), y.to(DEVICE))\n","            pred = unet(x)\n","            loss, acc = metric_lossFunc(pred.to(torch.float32), y.to(torch.float32)).item()\n","            totalValidationLoss_metric += loss\n","            totalAccuracy += acc\n","            \n","    # calculate the average training and validation loss and binary accuracy\n","    avgTrainLoss = totalTrainLoss / trainSteps\n","    avgValidationLoss = totalValidationLoss_metric / validationSteps\n","    avgAcc = totalAccuracy / validationSteps\n","    \n","    H[\"train_loss\"].append(avgTrainLoss)\n","    H[\"validation_loss\"].append(avgValidationLoss) \n","    H[\"model_accuracy\"].append(avgAcc) \n","    \n","    # print the model training and validation information\n","    print(f\"For EPOCH: {epoch + 1}/{NUM_EPOCHS}, and model goes through data->{len(trainLoader)} times)\")\n","    print(f\"Train loss: {avgTrainLoss:.6f}, Validation loss: {avgValidationLoss:.4f},Val accuracy: {avgAcc:.4f}%\")\n","    gpu_usage()\n","    \n","    # saving the intermidiary informations\n","    checkpoint_inference(model, epoch, optimizer, filename, newModel = None)\n","\n","def checkpoint_inference(model, epoch, optimizer, filename, newModel = None):\n","    \n","    if newModel == True:\n","          totalEpochs = \n","          \n","    print(\"=> Saving all model parameters \")\n","    filename = filename + '_ModelInference.pth'\n","    torch.save({\n","            'model': model,\n","            'model_state_dict': model.state_dict(),\n","            'epoch': epoch+1,\n","            'optimizer_state_dict': optimizer.state_dict()}, filename)\n","\n","    # terminate the training loop\n","#     if avgAcc > best_accuracy:\n","#         best_accuracy = avgAcc\n","#         best_epoch = epoch\n","#         checkpoint(unet, MODEL_PATH)\n","#     elif epoch - best_epoch > EARLY_STOP_THRES:\n","#         print(\"Early stopped training at epoch %d\" % epoch)\n","#         break  \n","    \n","    # Step the scheduler based on training loss \n","#     scheduler.step(avgTrainLoss)\n","    # Step the scheduler based on validation loss \n","#     scheduler.step(avgValidationLoss)\n","    \n","# display the total time needed to perform the training\n","endTime = time.time()\n","print(\"[INFO] total time taken to train the model: {:.2f}s\".format(endTime - startTime))"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-02-25T13:19:00.03046Z","iopub.status.busy":"2024-02-25T13:19:00.030009Z","iopub.status.idle":"2024-02-25T13:19:00.079193Z","shell.execute_reply":"2024-02-25T13:19:00.077629Z","shell.execute_reply.started":"2024-02-25T13:19:00.030426Z"},"trusted":true},"outputs":[],"source":["# save the final model and check_point inference if the mdel training completes with no issues\n","MODEL_PATH = save_checkpoint(unet, MODEL_PATH)\n","checkpoint_inference(unet,  epoch, optimizer, MODEL_PATH)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["torch.save(unet, MODEL_PATH)"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-02-20T15:23:36.5722Z","iopub.status.busy":"2024-02-20T15:23:36.571912Z","iopub.status.idle":"2024-02-20T15:23:36.928916Z","shell.execute_reply":"2024-02-20T15:23:36.927992Z","shell.execute_reply.started":"2024-02-20T15:23:36.572175Z"},"trusted":true},"outputs":[],"source":["\n","plt.style.use(\"ggplot\")\n","plt.figure()\n","plt.plot(acc_loss_info[\"train_loss\"], label=\"train_loss\")\n","plt.plot(acc_loss_info[\"validation_loss\"], label=\"validation_loss\")\n","plt.plot(acc_loss_info[\"model_accuracy\"], label=\"accuracy\")\n","plt.title(f\"Losses for version:-\\n{plotName}\")\n","plt.xlabel(\"Epoch #\")\n","plt.ylabel(\"Loss\")\n","plt.legend(loc=\"lower left\")\n","\n"]},{"cell_type":"markdown","metadata":{},"source":["# Model used for prediction of segemnted images"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-02-25T08:00:24.838138Z","iopub.status.busy":"2024-02-25T08:00:24.837275Z","iopub.status.idle":"2024-02-25T08:00:24.844983Z","shell.execute_reply":"2024-02-25T08:00:24.844097Z","shell.execute_reply.started":"2024-02-25T08:00:24.838109Z"},"trusted":true},"outputs":[],"source":["\n","def prepare_plot(origImage, origMask, predMask, file, threshold = None):\n","    # initialize our figure\n","    figure, ax = plt.subplots(nrows=1, ncols=3, figsize=(15, 15))\n","    ax[0].imshow(origImage)\n","    ax[1].imshow(origMask)\n","    ax[2].imshow(predMask)\n","#     ax[3].imshow(predMask > (predMask.max() - threshold))\n","#     ax[3].hist(predMask.flatten()*255)\n","\n","    ax[0].set_title(f\"{file}\")\n","    ax[1].set_title(\"Original Mask\")\n","    ax[2].set_title(f\"P_Mask-{threshold}thresh\")\n","#     ax[3].set_title('P_mask w threshold')\n","\n","    figure.tight_layout()\n","    figure.show()"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["with torch.no_grad():\n","        unet.eval()\n","        for (x, y) in validationLoader:\n","            \"\"\" Note make sure to change the i/p channel to later on ([-1, 1, patch_size, patch_size])\"\"\"\n","            x = torch.reshape(x,[-1,1,PATCH_SIZE,PATCH_SIZE]).float()\n","            y = torch.reshape(y,[-1,1,PATCH_SIZE,PATCH_SIZE]).float()\n","            (x, y) = (x.to(DEVICE), y.to(DEVICE))\n","            pred = unet(x)\n","            loss, acc = metric_lossFunc(pred.to(torch.float32), y.to(torch.float32)).item()\n","            totalValidationLoss_metric += loss\n","            totalAccuracy += acc"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["device = torch.device(\"cuda:1\" if torch.cuda.is_available() else \"cpu\")\n","device "]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-02-25T08:02:10.455635Z","iopub.status.busy":"2024-02-25T08:02:10.455252Z","iopub.status.idle":"2024-02-25T08:02:10.466888Z","shell.execute_reply":"2024-02-25T08:02:10.466033Z","shell.execute_reply.started":"2024-02-25T08:02:10.455607Z"},"trusted":true},"outputs":[],"source":["def make_predictions(model, file, imagePath, groundTruthPath, patch_size, threshold = None):\n","    \n","    # model.to('cuda:0')\n","    model.eval()\n","    with torch.no_grad():\n","        image = Image.open(imagePath).convert(\"L\")\n","        image = np.float32(image)/255.0\n","        \n","        gtMask = np.array(Image.open(groundTruthPath).convert(\"L\"))\n","        gtMask[gtMask > 0] = 1.0\n","    \n","        height, width = image.shape[:2]\n","        segm_img = np.zeros((height, width), dtype=np.uint8)  # Array with zeros to be filled with segmented values\n","    \n","        patch_num = 1\n","    \n","        for i in range(0, height, patch_size):  \n","            for j in range(0, width, patch_size):  \n","                single_patch = image[i:i+patch_size, j:j+patch_size]\n","                single_patch = np.expand_dims(np.expand_dims(single_patch, 0), 0)\n","                orig_patch = single_patch.copy()\n","                single_patch = torch.from_numpy(single_patch).to(DEVICE)\n","                \n","                Mask_patch = gtMask[i:i+patch_size, j:j+patch_size]\n","               \n","                # pass the results through the sigmoid if the last layer of the model doesnot do sigmoid conversion\n","                # single_patch_prediction = torch.sigmoid(model(single_patch)).squeeze().cpu().numpy()\n","                single_patch_prediction = model(single_patch).squeeze().cpu().numpy()\n","                \n","                # filter out the weak predictions and convert them to integers\n","                single_patch_prediction = ((single_patch_prediction>threshold)*255).astype(np.uint8)\n","                \n","                single_patch_shape = single_patch_prediction.shape[:2]\n","                segm_img[i:i+single_patch_shape[0], j:j+single_patch_shape[1]] += cv2.resize(single_patch_prediction, single_patch_shape[::-1])\n","                # print(\"Finished processing patch number \", patch_num, \" at position \", i, j)\n","            \n","            patch_num += 1  \n","    # print(f\"Finised-{file}\")\n","    prepare_plot(image, gtMask, segm_img, file, threshold)\n","    return single_patch_prediction"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-02-25T08:02:26.909139Z","iopub.status.busy":"2024-02-25T08:02:26.90827Z","iopub.status.idle":"2024-02-25T08:02:34.036837Z","shell.execute_reply":"2024-02-25T08:02:34.035867Z","shell.execute_reply.started":"2024-02-25T08:02:26.909108Z"},"trusted":true},"outputs":[],"source":["\n","print(\"[INFO] load up model...\")\n","## clear the GPU used \n","Device = 'cuda:2' # or tery other devoces  if either of rhe devices are blocked\n","unet = torch.load(MODEL_PATH).to(Device)\n","print(\"[INFO] loading up test image paths...\\n\")\n","test_input = '/user/s.chakrabarty/myoTwinWork/data/embryonicImages/test_inputs'\n","test_mask = '/user/s.chakrabarty/myoTwinWork/data/embryonicImages/test_masks'\n","\n","files = os.listdir(test_input)\n","for file in files:\n","    if file == '.DS_Store': continue\n","    imagePath = test_input+ '/' + file\n","    groundTruthPath = test_mask + '/' + '.'.join(file.split('.')[:-1]) + '_bn.tif'\n","    simg = make_predictions(unet, file, imagePath, groundTruthPath, PATCH_SIZE, threshold = 0.5)\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-02-22T09:07:35.553101Z","iopub.status.busy":"2024-02-22T09:07:35.552565Z","iopub.status.idle":"2024-02-22T09:07:39.73638Z","shell.execute_reply":"2024-02-22T09:07:39.734947Z","shell.execute_reply.started":"2024-02-22T09:07:35.553062Z"},"trusted":true},"outputs":[],"source":["gpu_usage()\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":[]}],"metadata":{"kaggle":{"accelerator":"none","dataSources":[{"datasetId":3448145,"sourceId":7669043,"sourceType":"datasetVersion"},{"datasetId":4157240,"sourceId":7690030,"sourceType":"datasetVersion"},{"modelInstanceId":9467,"sourceId":11724,"sourceType":"modelInstanceVersion"},{"isSourceIdPinned":true,"modelInstanceId":9707,"sourceId":11985,"sourceType":"modelInstanceVersion"}],"isGpuEnabled":false,"isInternetEnabled":true,"language":"python","sourceType":"notebook"},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.11.2"}},"nbformat":4,"nbformat_minor":4}
